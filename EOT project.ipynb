{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2021\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n",
    "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n",
    "# persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n",
    "# Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n",
    "# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\"\"\"\n",
    "This module defines a base class for EoT in PyTorch.\n",
    "\"\"\"\n",
    "from abc import abstractmethod\n",
    "import logging\n",
    "from typing import Optional, Tuple, TYPE_CHECKING, List\n",
    "\n",
    "from art.preprocessing.preprocessing import PreprocessorPyTorch\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EoTPyTorch(PreprocessorPyTorch):\n",
    "    \"\"\"\n",
    "    This module defines a base class for EoT in PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_samples: int,\n",
    "        clip_values: Tuple[float, float],\n",
    "        apply_fit: bool = False,\n",
    "        apply_predict: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create an instance of EoTPyTorch.\n",
    "\n",
    "        :param nb_samples: Number of random samples per input sample.\n",
    "        :param clip_values: Tuple of float representing minimum and maximum values of input `(min, max)`.\n",
    "        :param apply_fit: True if applied during fitting/training.\n",
    "        :param apply_predict: True if applied during predicting.\n",
    "        \"\"\"\n",
    "        super().__init__(is_fitted=True, apply_fit=apply_fit, apply_predict=apply_predict)\n",
    "\n",
    "        self.nb_samples = nb_samples\n",
    "        self.clip_values = clip_values\n",
    "        EoTPyTorch._check_params(self)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _transform(\n",
    "        self, x: \"torch.Tensor\", y: Optional[\"torch.Tensor\"], **kwargs\n",
    "    ) -> Tuple[\"torch.Tensor\", Optional[\"torch.Tensor\"]]:\n",
    "        \"\"\"\n",
    "        Internal method implementing the transformation per input sample.\n",
    "\n",
    "        :param x: Input samples.\n",
    "        :param y: Label of the samples `x`.\n",
    "        :return: Transformed samples and labels.\n",
    "        \"\"\"\n",
    "        \"The transform will change the input to make it more adversarially robust.\"\n",
    "        import cv2\n",
    "        #tensor_numpy= x.cpu().numpy() # make sure tensor is on cpu\n",
    "        tensor_to_numpy = x.numpy()   \n",
    "        cv2.imwrite(tensor_to_numpy, \"the_clone.jpg\")\n",
    "        # edit the image\n",
    "        from matplotlib import image\n",
    "        from matplotlib import pyplot as plt\n",
    "\n",
    "        # to read the image stored in the working directory\n",
    "        data = image.imread('the_clone.jpg')\n",
    "        #parameters of height and width\n",
    "        width = data.shape[2] \n",
    "        height = data.shape[1]\n",
    "        x1 = random.randint(0, width)\n",
    "        x2 = random.randint(x1, width)\n",
    "        y1 = random.randint(0, width)\n",
    "        y2 = random.randint(y1, width)\n",
    "        x_line = [x1, x2]\n",
    "        y_line = [y1, y2]\n",
    "        plt.plot(x_line, y_line, color=\"white\", linewidth=3)\n",
    "        plt.axis('off')\n",
    "        #now convert the figure to jpeg, then to a tensor\n",
    "        plt.savefig('the_clone.jpg')\n",
    "        from torchvision.io.ImageReadMode import read_image\n",
    "        tensor = read_image(\"the_clone.jpg\") #by default is ImageReadMode.Unchanged\n",
    "        return torch.clamp(tensor, min=self.clip_values[0], max=self.clip_values[1]), y\n",
    "\n",
    "    def forward(\n",
    "        self, x: \"torch.Tensor\", y: Optional[\"torch.Tensor\"] = None\n",
    "    ) -> Tuple[\"torch.Tensor\", Optional[\"torch.Tensor\"]]:\n",
    "        \"\"\"\n",
    "        Apply transformations to inputs `x` and labels `y`.\n",
    "\n",
    "        :param x: Input samples.\n",
    "        :param y: Label of the samples `x`.\n",
    "        :return: Transformed samples and labels.\n",
    "        \"\"\"\n",
    "        import torch  # lgtm [py/repeated-import]\n",
    "\n",
    "        x_preprocess_list = list()\n",
    "        y_preprocess_list: List[\"torch.Tensor\"] = list()\n",
    "\n",
    "        for i_image in range(x.shape[0]):\n",
    "            for _ in range(self.nb_samples):\n",
    "                x_i = x[i_image]\n",
    "                y_i: Optional[\"torch.Tensor\"]\n",
    "                if y is not None:\n",
    "                    y_i = y[i_image]\n",
    "                else:\n",
    "                    y_i = None\n",
    "                x_preprocess, y_preprocess_i = self._transform(x_i, y_i)\n",
    "                x_preprocess_list.append(x_preprocess)\n",
    "\n",
    "                if y is not None and y_preprocess_i is not None:\n",
    "                    y_preprocess_list.append(y_preprocess_i)\n",
    "\n",
    "        x_preprocess = torch.stack(x_preprocess_list, dim=0)\n",
    "        if y is None:\n",
    "            y_preprocess = y\n",
    "        else:\n",
    "            y_preprocess = torch.stack(y_preprocess_list, dim=0)\n",
    "\n",
    "        return x_preprocess, y_preprocess\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "\n",
    "        if not isinstance(self.nb_samples, int) or self.nb_samples < 1:\n",
    "            raise ValueError(\"The number of samples needs to be an integer greater than or equal to 1.\")\n",
    "\n",
    "        if not isinstance(self.clip_values, tuple) or (\n",
    "            len(self.clip_values) != 2\n",
    "            or not isinstance(self.clip_values[0], (int, float))\n",
    "            or not isinstance(self.clip_values[1], (int, float))\n",
    "            or self.clip_values[0] > self.clip_values[1]\n",
    "        ):\n",
    "            raise ValueError(\"The argument `clip_Values` has to be a float or tuple of two float values as (min, max).\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
